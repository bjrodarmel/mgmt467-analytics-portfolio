{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bjrodarmel/mgmt467-analytics-portfolio/blob/main/Unit2_Lab1_Finished.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXAxRqR3kEdh"
      },
      "source": [
        "# MGMT 467 — Prompt-Driven Lab (with Commented Examples)\n",
        "## Kaggle ➜ Google Cloud Storage ➜ BigQuery ➜ Data Quality (DQ)\n",
        "\n",
        "**How to use this notebook**\n",
        "- Each section gives you a **Build Prompt** to paste into Gemini/Vertex AI (or Gemini in Colab).\n",
        "- Below each prompt, you’ll see a **commented example** of what a good LLM answer might look like.\n",
        "- **Do not** just uncomment and run. Use the prompt to generate your own code, then compare to the example.\n",
        "- After every step, run the **Verification Prompt**, and write the **Reflection** in Markdown.\n",
        "\n",
        "> Goal today: Download the Netflix dataset (Kaggle) → Stage on GCS → Load into BigQuery → Run DQ profiling (missingness, duplicates, outliers, anomaly flags).\n"
      ],
      "id": "pXAxRqR3kEdh"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oEfoA-WWkEdj"
      },
      "source": [
        "### Academic integrity & LLM usage\n",
        "- Use the prompts here to generate your own code cells.\n",
        "- Read concept notes and write the reflection answers in your own words.\n",
        "- Keep credentials out of code. Upload `kaggle.json` when asked.\n"
      ],
      "id": "oEfoA-WWkEdj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufmGPNXvkEdj"
      },
      "source": [
        "## Learning objectives\n",
        "1) Explain **why** we stage data in GCS and load it to BigQuery.  \n",
        "2) Build an **idempotent**, auditable pipeline.  \n",
        "3) Diagnose **missingness**, **duplicates**, and **outliers** and justify cleaning choices.  \n",
        "4) Connect DQ decisions to **business/ML impact**.\n"
      ],
      "id": "ufmGPNXvkEdj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRPkWvbmkEdj"
      },
      "source": [
        "## 0) Environment setup — What & Why\n",
        "Authenticate Colab to Google Cloud so we can use `gcloud`, GCS, and BigQuery. Set **PROJECT_ID** and **REGION** once for consistency (cost/latency)."
      ],
      "id": "tRPkWvbmkEdj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ks4Rg6oVkEdj"
      },
      "source": [
        "### Build Prompt (paste to LLM)\n",
        "You are my cloud TA. Generate a single **Colab code cell** that:\n",
        "1) Authenticates to Google Cloud in Colab,  \n",
        "2) Prompts for `PROJECT_ID` via `input()` and sets `REGION=\"us-central1\"` (editable),  \n",
        "3) Exports `GOOGLE_CLOUD_PROJECT`,  \n",
        "4) Runs `gcloud config set project $GOOGLE_CLOUD_PROJECT`,  \n",
        "5) Prints both values. Add 2–3 comments explaining what/why.\n",
        "End with a comment: `# Done: Auth + Project/Region set`.\n"
      ],
      "id": "ks4Rg6oVkEdj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pTE7WvkpkEdj",
        "outputId": "76a95377-28dc-4cce-efac-3483381f0a4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING: google.colab.auth.authenticate_user() is not supported in Colab Enterprise.\n",
            "Enter your Google Cloud Project ID: mgmt-467\n",
            "Updated property [core/project].\n",
            "Project ID: mgmt-467\n",
            "Region: us-central1\n"
          ]
        }
      ],
      "source": [
        "# prompt: You are my cloud TA. Generate a single Colab code cell that:\n",
        "# Authenticates to Google Cloud in Colab,\n",
        "# Prompts for PROJECT_ID via input() and sets REGION=\"us-central1\" (editable),\n",
        "# Exports GOOGLE_CLOUD_PROJECT,\n",
        "# Runs gcloud config set project $GOOGLE_CLOUD_PROJECT,\n",
        "# Prints both values. Add 2–3 comments explaining what/why. End with a comment: # Done: Auth + Project/Region set.\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Prompt for project ID and set region\n",
        "PROJECT_ID = input(\"Enter your Google Cloud Project ID: \")\n",
        "REGION = \"us-central1\"  # Editable\n",
        "\n",
        "# Export project ID as an environment variable\n",
        "import os\n",
        "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = PROJECT_ID\n",
        "\n",
        "# Configure gcloud with the project ID\n",
        "!gcloud config set project $GOOGLE_CLOUD_PROJECT\n",
        "\n",
        "# Print the configured project ID and region\n",
        "print(f\"Project ID: {PROJECT_ID}\")\n",
        "print(f\"Region: {REGION}\")\n",
        "\n",
        "# Done: Auth + Project/Region set\n"
      ],
      "id": "pTE7WvkpkEdj"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4OUpiA9kEdk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a short cell that prints the active project using `gcloud config get-value project` and echoes the `REGION` you set.\n"
      ],
      "id": "y4OUpiA9kEdk"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a short cell that prints the active project using gcloud config get-value project and echoes the REGION you set.\n",
        "\n",
        "print(f\"Active Project: {get_ipython().getoutput('gcloud config get-value project')[0]}\")\n",
        "print(f\"Region: {REGION}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fOtKisH8kXZ_",
        "outputId": "73c54c59-45b8-45cc-ae29-ffc550a72abd"
      },
      "id": "fOtKisH8kXZ_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Active Project: mgmt-467\n",
            "Region: us-central1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6WobLbZkEdk"
      },
      "source": [
        "**Reflection:** Why do we set `PROJECT_ID` and `REGION` at the top? What can go wrong if we don’t?"
      ],
      "id": "Q6WobLbZkEdk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setting `PROJECT_ID` and `REGION` at the top of the notebook is crucial for several reasons:\n",
        "\n",
        "1.  **Consistency and Reproducibility:** It ensures that all subsequent operations (like creating GCS buckets, running BigQuery jobs, or deploying resources) are performed within the same project and region. This makes the notebook's execution predictable and reproducible. If different parts of the notebook were to implicitly or explicitly use different project IDs or regions, it could lead to confusion, data being placed in unexpected locations, or errors due to resource availability or permissions.\n",
        "\n",
        "2.  **Cost Management:** Google Cloud resources are billed based on usage within a specific project. By defining the `PROJECT_ID` upfront, you ensure all costs are attributed to the intended project. Similarly, `REGION` selection can impact costs (e.g., data transfer costs between regions) and latency. Setting it once prevents accidental cross-region operations that might incur higher costs or performance penalties.\n",
        "\n",
        "3.  **Resource Scoping and Permissions:** Google Cloud IAM (Identity and Access Management) policies are often scoped to projects and resources within specific regions. By setting these values, you ensure that the authenticated user or service account has the necessary permissions to interact with resources in that project and region. If not set, the system might default to a project or region that the user doesn't have access to, leading to permission denied errors.\n",
        "\n",
        "4.  **Clarity and Readability:** It makes the notebook's intent clear to anyone reading it. It's immediately obvious which project and region the notebook is configured to work with, reducing the cognitive load for collaborators or for your future self.\n",
        "\n",
        "**What can go wrong if we don’t set `PROJECT_ID` and `REGION`?**\n",
        "\n",
        "*   **Incorrect Project/Region Usage:** Operations might default to a project or region that is not intended, leading to data being stored or processed in the wrong place. This can cause data silos, compliance issues, or make it difficult to find your data later.\n",
        "*   **Permission Errors:** If the default project or region is one where your credentials lack sufficient permissions, many operations will fail with authorization errors.\n",
        "*   **Unexpected Costs:** Operations might inadvertently run in a more expensive region or incur data egress charges if not explicitly controlled.\n",
        "*   **Data Locality and Latency Issues:** If data is processed in a region far from where it's stored or where users are accessing it, it can lead to significant latency.\n",
        "*   **Confusion and Debugging Difficulties:** When errors occur, it becomes much harder to debug if you're unsure which project or region the failing operation was targeting.\n",
        "*   **Inability to Create Resources:** Some resources (like certain BigQuery datasets or GCS buckets) are regional or global. Without a clear project and region, the creation process might fail or create resources in an unintended global scope."
      ],
      "metadata": {
        "id": "8NNR0CXmkhaw"
      },
      "id": "8NNR0CXmkhaw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zE_dTPEAkEdk"
      },
      "source": [
        "## 1) Kaggle API — What & Why\n",
        "Use Kaggle CLI for reproducible downloads. Store `kaggle.json` at `~/.kaggle/kaggle.json` with `0600` permissions to protect secrets."
      ],
      "id": "zE_dTPEAkEdk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUuuydYgkEdk"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **single Colab code cell** that:\n",
        "- Prompts me to upload `kaggle.json`,\n",
        "- Saves to `~/.kaggle/kaggle.json` with `0600` permissions,\n",
        "- Prints `kaggle --version`.\n",
        "Add comments about security and reproducibility.\n"
      ],
      "id": "nUuuydYgkEdk"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "-0kDRI9AkEdk",
        "outputId": "898e45b5-80ef-4f8d-b814-2eca254fb29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please upload your kaggle.json file\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a41a36d2-2f48-42df-b6d0-1c05dc040f45\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a41a36d2-2f48-42df-b6d0-1c05dc040f45\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n",
            "Kaggle API 1.7.4.5\n"
          ]
        }
      ],
      "source": [
        "# prompt: Generate a single Colab code cell that:\n",
        "# Prompts me to upload kaggle.json,\n",
        "# Saves to ~/.kaggle/kaggle.json with 0600 permissions,\n",
        "# Prints kaggle --version. Add comments about security and reproducibility.\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Prompt the user to upload their Kaggle API credentials file.\n",
        "# This file contains sensitive information and should be kept secure.\n",
        "print(\"Please upload your kaggle.json file\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Create the .kaggle directory if it doesn't exist.\n",
        "# The Kaggle CLI looks for credentials in this specific folder.\n",
        "os.makedirs('/root/.kaggle', exist_ok=True)\n",
        "\n",
        "# Move the uploaded kaggle.json to the .kaggle directory.\n",
        "!mv kaggle.json /root/.kaggle/\n",
        "\n",
        "# Set the permissions of the file to 600 (read/write for owner only).\n",
        "# This is a security measure to protect your API key.\n",
        "!chmod 600 /root/.kaggle/kaggle.json\n",
        "\n",
        "# Print the Kaggle version to verify that the CLI is installed and configured correctly.\n",
        "# This confirms that the authentication setup was successful.\n",
        "!kaggle --version"
      ],
      "id": "-0kDRI9AkEdk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NYVPjA5lkEdk"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a one-liner that runs `kaggle --help | head -n 20` to show the CLI is ready.\n"
      ],
      "id": "NYVPjA5lkEdk"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a one-liner that runs kaggle --help | head -n 20 to show the CLI is ready.\n",
        "\n",
        "!kaggle --help | head -n 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fz0R399Mkzkv",
        "outputId": "2adb8bec-2daa-4713-c95d-f869ab4407d5"
      },
      "id": "fz0R399Mkzkv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: kaggle [-h] [-v] [-W]\n",
            "              {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "              ...\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  -v, --version         Print the Kaggle API version\n",
            "  -W, --no-warn         Disable out-of-date API version warning\n",
            "\n",
            "commands:\n",
            "  {competitions,c,datasets,d,kernels,k,models,m,files,f,config}\n",
            "                        Use one of:\n",
            "                        competitions {list, files, download, submit, submissions, leaderboard}\n",
            "                        datasets {list, files, download, create, version, init, metadata, status}\n",
            "                        kernels {list, files, init, push, pull, output, status}\n",
            "                        models {instances, get, list, init, create, delete, update}\n",
            "                        models instances {versions, get, files, init, create, delete, update}\n",
            "                        models instances versions {init, create, download, delete, files}\n",
            "                        config {view, set, unset}\n",
            "    competitions (c)    Commands related to Kaggle competitions\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhYqBitDkEdk"
      },
      "source": [
        "**Reflection:** Why require strict `0600` permissions on API tokens? What risks are we avoiding?"
      ],
      "id": "XhYqBitDkEdk"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reflection: Why require strict 0600 permissions on API tokens? What risks are we avoiding?\n",
        "Requiring strict `0600` permissions on API tokens (like `kaggle.json`) is a critical security measure.\n",
        "#\n",
        "**Why `0600` permissions?**\n",
        "The `0600` permission mode in Unix-like systems means:\n",
        "- The owner of the file can read (`6`) and write (`0`) to it.\n",
        "- No one else (group or others) has any permissions (`0`).\n",
        "\n",
        "**Risks we are avoiding:**\n",
        "#\n",
        "1.  **Unauthorized Access and Data Breaches:** If the API token file is readable by other users on the system (e.g., `0644` or `0666`), any user with access to the same machine or environment could potentially read the token. This token often grants access to your accounts on services like Kaggle, allowing them to download your data, impersonate you, or perform actions on your behalf.\n",
        "#\n",
        "2.  **Account Compromise:** An API token is essentially a form of authentication. If it falls into the wrong hands, attackers can use it to gain unauthorized access to your Kaggle account, potentially leading to the compromise of your personal information, private datasets, or even the ability to manipulate your account settings.\n",
        "#\n",
        "3.  **Data Leakage:** If the token is used to download data, and the token itself is leaked, the data that was accessed using that token could also be considered compromised or at risk.\n",
        "#\n",
        "4.  **Malicious Use:** An attacker could use your API token to perform actions on Kaggle that violate their terms of service, potentially putting your account at risk of suspension or leading to other negative consequences.\n",
        "#\n",
        "In essence, `0600` permissions ensure that only the owner of the file (which should be you, the user running the notebook) can access the sensitive credentials. This minimizes the attack surface and protects your account and data from unauthorized access and misuse."
      ],
      "metadata": {
        "id": "EP9A_STFk9wU"
      },
      "id": "EP9A_STFk9wU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uEL5e7VkEdk"
      },
      "source": [
        "## 2) Download & unzip dataset — What & Why\n",
        "Keep raw files under `/content/data/raw` for predictable paths and auditing.\n",
        "**Dataset:** `sayeeduddin/netflix-2025user-behavior-dataset-210k-records`"
      ],
      "id": "6uEL5e7VkEdk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V105jXgHkEdl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates `/content/data/raw`,\n",
        "- Downloads the dataset to `/content/data` with Kaggle CLI,\n",
        "- Unzips into `/content/data/raw` (overwrite OK),\n",
        "- Lists all CSVs with sizes in a neat table.\n",
        "Include comments describing each step.\n"
      ],
      "id": "V105jXgHkEdl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ilx9KTqpkEdl",
        "outputId": "c69fbe92-fd17-4c20-c958-c11cbede1025"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/sayeeduddin/netflix-2025user-behavior-dataset-210k-records\n",
            "License(s): CC0-1.0\n",
            "Downloading netflix-2025user-behavior-dataset-210k-records.zip to /content/data\n",
            "  0% 0.00/4.02M [00:00<?, ?B/s]\n",
            "100% 4.02M/4.02M [00:00<00:00, 575MB/s]\n",
            "Archive:  /content/data/netflix-2025user-behavior-dataset-210k-records.zip\n",
            "  inflating: /content/data/raw/README.md  \n",
            "  inflating: /content/data/raw/movies.csv  \n",
            "  inflating: /content/data/raw/recommendation_logs.csv  \n",
            "  inflating: /content/data/raw/reviews.csv  \n",
            "  inflating: /content/data/raw/search_logs.csv  \n",
            "  inflating: /content/data/raw/users.csv  \n",
            "  inflating: /content/data/raw/watch_history.csv  \n",
            "-rw-r--r-- 1 root root 114K Aug  2 19:36 /content/data/raw/movies.csv\n",
            "-rw-r--r-- 1 root root 4.5M Aug  2 19:36 /content/data/raw/recommendation_logs.csv\n",
            "-rw-r--r-- 1 root root 1.8M Aug  2 19:36 /content/data/raw/reviews.csv\n",
            "-rw-r--r-- 1 root root 2.2M Aug  2 19:36 /content/data/raw/search_logs.csv\n",
            "-rw-r--r-- 1 root root 1.6M Aug  2 19:36 /content/data/raw/users.csv\n",
            "-rw-r--r-- 1 root root 8.9M Aug  2 19:36 /content/data/raw/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "# prompt: Generate a Colab code cell that:\n",
        "# Creates /content/data/raw,\n",
        "# Downloads the dataset to /content/data with Kaggle CLI,\n",
        "# Unzips into /content/data/raw (overwrite OK),\n",
        "# Lists all CSVs with sizes in a neat table. Include comments describing each step.\n",
        "\n",
        "# Create the directory to store raw data\n",
        "!mkdir -p /content/data/raw\n",
        "\n",
        "# Download the dataset using Kaggle CLI to /content/data\n",
        "# The -d flag specifies the dataset, and -p specifies the download path\n",
        "!kaggle datasets download -d sayeeduddin/netflix-2025user-behavior-dataset-210k-records -p /content/data\n",
        "\n",
        "# Unzip the downloaded dataset into the raw data directory\n",
        "# -o flag overwrites files if they exist\n",
        "!unzip -o /content/data/*.zip -d /content/data/raw\n",
        "\n",
        "# List all CSV files in the raw data directory with their sizes in a neat table\n",
        "!ls -lh /content/data/raw/*.csv\n"
      ],
      "id": "Ilx9KTqpkEdl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ntuP5M-QkEdl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that asserts there are exactly **six** CSV files and prints their names.\n"
      ],
      "id": "ntuP5M-QkEdl"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a snippet that asserts there are exactly six CSV files and prints their names.\n",
        "\n",
        "import glob\n",
        "\n",
        "csv_files = glob.glob('/content/data/raw/*.csv')\n",
        "assert len(csv_files) == 6, f\"Expected 6 CSV files, but found {len(csv_files)}\"\n",
        "\n",
        "print(\"Found CSV files:\")\n",
        "for f in csv_files:\n",
        "    print(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6aJrPVWhl5Rp",
        "outputId": "49d8729c-b865-4e3f-e02b-4d3963dc25b8"
      },
      "id": "6aJrPVWhl5Rp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found CSV files:\n",
            "/content/data/raw/watch_history.csv\n",
            "/content/data/raw/recommendation_logs.csv\n",
            "/content/data/raw/search_logs.csv\n",
            "/content/data/raw/users.csv\n",
            "/content/data/raw/reviews.csv\n",
            "/content/data/raw/movies.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDJHrLQ6kEdl"
      },
      "source": [
        "**Reflection:** Why is keeping a clean file inventory (names, sizes) useful downstream?"
      ],
      "id": "nDJHrLQ6kEdl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reflection: Why is keeping a clean file inventory (names, sizes) useful downstream?\n",
        "Keeping a clean file inventory, including file names and their sizes, is crucial for several downstream tasks in a data pipeline:\n",
        "#\n",
        "1.  **Data Validation and Integrity Checks:**\n",
        "     *   **Completeness:** Knowing the expected number of files and their names allows you to quickly verify if all expected data has been downloaded or generated. If a file is missing, it's an immediate red flag.\n",
        "     *   **Size Verification:** Comparing the downloaded file sizes against expected sizes (or previous runs) can help detect corruption or incomplete downloads. A file that is significantly smaller than expected might indicate an interrupted download or a truncated file.\n",
        "#\n",
        "2.  **Pipeline Automation and Reproducibility:**\n",
        "     *   **Automated Processing:** Scripts can be written to automatically process all files matching a certain pattern (e.g., all `.csv` files in a directory). A consistent inventory ensures these scripts run reliably.\n",
        "     *   **Reproducibility:** For reproducible research or analysis, it's essential to know exactly which files were used. Documenting the file inventory at each stage of the pipeline allows others (or your future self) to recreate the exact dataset used.\n",
        "#\n",
        " 3.  **Resource Management and Monitoring:**\n",
        "     *   **Storage Planning:** Knowing the total size of the data helps in planning storage requirements and managing costs, especially in cloud environments.\n",
        "     *   **Performance Monitoring:** Large files can impact processing times. Understanding file sizes helps in estimating processing durations and identifying potential bottlenecks.\n",
        "#\n",
        " 4.  **Debugging and Troubleshooting:**\n",
        "     *   **Error Localization:** If a processing step fails, having a clear inventory of the input files helps pinpoint which file might be causing the issue (e.g., if it's malformed, empty, or unexpectedly large).\n",
        "     *   **Auditing:** In case of discrepancies or unexpected results, the file inventory serves as an audit trail, showing the state of the data at different points in time.\n",
        "#\n",
        " 5.  **Data Understanding and Exploration:**\n",
        "     *   **Initial Assessment:** A quick glance at file names and sizes can provide initial insights into the structure and scale of the dataset, helping to guide the next steps of exploration and analysis.\n",
        "#\n",
        " In summary, a clean file inventory acts as a foundational element for robust, automated, and understandable data pipelines. It transforms a collection of files into a manageable and verifiable dataset.\n"
      ],
      "metadata": {
        "id": "iCrrrSSFmWSc"
      },
      "id": "iCrrrSSFmWSc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7uDIuRy1kEdl"
      },
      "source": [
        "## 3) Create GCS bucket & upload — What & Why\n",
        "Stage in GCS → consistent, versionable source for BigQuery loads. Bucket names must be **globally unique**."
      ],
      "id": "7uDIuRy1kEdl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0vwZzM-kEdl"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a **Colab code cell** that:\n",
        "- Creates a unique bucket in `${REGION}` (random suffix),\n",
        "- Saves name to `BUCKET_NAME` env var,\n",
        "- Uploads all CSVs to `gs://$BUCKET_NAME/netflix/`,\n",
        "- Prints the bucket name and explains staging benefits.\n"
      ],
      "id": "U0vwZzM-kEdl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imhz-lu-kEdl",
        "outputId": "6b8ad184-922a-4b55-8bdd-4b95e6a5662d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating bucket: gs://mgmt467-netflix-379f8b03 in region: us-central1\n",
            "Creating gs://mgmt467-netflix-379f8b03/...\n",
            "\n",
            "Uploading CSV files to gs://mgmt467-netflix-379f8b03/netflix/\n",
            "Copying file:///content/data/raw/movies.csv to gs://mgmt467-netflix-379f8b03/netflix/movies.csv\n",
            "Copying file:///content/data/raw/recommendation_logs.csv to gs://mgmt467-netflix-379f8b03/netflix/recommendation_logs.csv\n",
            "Copying file:///content/data/raw/reviews.csv to gs://mgmt467-netflix-379f8b03/netflix/reviews.csv\n",
            "Copying file:///content/data/raw/search_logs.csv to gs://mgmt467-netflix-379f8b03/netflix/search_logs.csv\n",
            "Copying file:///content/data/raw/users.csv to gs://mgmt467-netflix-379f8b03/netflix/users.csv\n",
            "Copying file:///content/data/raw/watch_history.csv to gs://mgmt467-netflix-379f8b03/netflix/watch_history.csv\n",
            "\n",
            "Average throughput: 62.5MiB/s\n",
            "\n",
            "Successfully created bucket: mgmt467-netflix-379f8b03 and uploaded files.\n",
            "\n",
            "Benefits of staging data in GCS:\n",
            "- **Consistent Source:** GCS provides a stable and versionable location for your data.\n",
            "- **Scalability:** GCS is highly scalable, handling large datasets easily.\n",
            "- **Integration:** GCS integrates seamlessly with other Google Cloud services like BigQuery.\n",
            "- **Cost-Effective:** GCS can be a cost-effective storage solution.\n",
            "\n",
            "Verifying contents of gs://mgmt467-netflix-379f8b03/netflix/\n",
            "gs://mgmt467-netflix-379f8b03/netflix/movies.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/users.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/watch_history.csv\n"
          ]
        }
      ],
      "source": [
        "import uuid\n",
        "import os\n",
        "\n",
        "# Generate a unique bucket name. Bucket names must be globally unique.\n",
        "bucket_name = f\"mgmt467-netflix-{uuid.uuid4().hex[:8]}\"\n",
        "os.environ[\"BUCKET_NAME\"] = bucket_name\n",
        "\n",
        "# The REGION variable was set in a previous cell. We use it to create the bucket\n",
        "# in the correct location for lower latency and cost.\n",
        "print(f\"Creating bucket: gs://{bucket_name} in region: {REGION}\")\n",
        "!gcloud storage buckets create gs://{bucket_name} --location={REGION}\n",
        "\n",
        "# Upload all CSVs from the local directory to the GCS bucket.\n",
        "# Staging data in GCS makes it a reliable and scalable source for BigQuery.\n",
        "print(f\"\\nUploading CSV files to gs://{bucket_name}/netflix/\")\n",
        "!gcloud storage cp /content/data/raw/*.csv gs://{bucket_name}/netflix/\n",
        "\n",
        "# Print the bucket name and explain staging benefits\n",
        "print(f\"\\nSuccessfully created bucket: {bucket_name} and uploaded files.\")\n",
        "print(\"\\nBenefits of staging data in GCS:\")\n",
        "print(\"- **Consistent Source:** GCS provides a stable and versionable location for your data.\")\n",
        "print(\"- **Scalability:** GCS is highly scalable, handling large datasets easily.\")\n",
        "print(\"- **Integration:** GCS integrates seamlessly with other Google Cloud services like BigQuery.\")\n",
        "print(\"- **Cost-Effective:** GCS can be a cost-effective storage solution.\")\n",
        "\n",
        "# Verify that the files have been uploaded to the bucket.\n",
        "print(f\"\\nVerifying contents of gs://{bucket_name}/netflix/\")\n",
        "!gcloud storage ls gs://{bucket_name}/netflix/"
      ],
      "id": "Imhz-lu-kEdl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "df7opTI5kEdl"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a snippet that lists the `netflix/` prefix and shows object sizes.\n"
      ],
      "id": "df7opTI5kEdl"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a snippet that lists the netflix/ prefix and shows object sizes.\n",
        "\n",
        "!gcloud storage ls --recursive gs://{os.environ['BUCKET_NAME']}/netflix/\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nPFPooySmfVh",
        "outputId": "50841c5d-a5eb-4be7-bf40-b2be2f3875f6"
      },
      "id": "nPFPooySmfVh",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "gs://mgmt467-netflix-379f8b03/netflix/:\n",
            "gs://mgmt467-netflix-379f8b03/netflix/movies.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/recommendation_logs.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/reviews.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/search_logs.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/users.csv\n",
            "gs://mgmt467-netflix-379f8b03/netflix/watch_history.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OC1ZCd0IkEdl"
      },
      "source": [
        "**Reflection:** Name two benefits of staging in GCS vs loading directly from local Colab."
      ],
      "id": "OC1ZCd0IkEdl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reflection: Name two benefits of staging in GCS vs loading directly from local Colab.\n",
        "#\n",
        " 1.  **Scalability and Durability:** Google Cloud Storage (GCS) is designed for massive scalability and high durability. Unlike the temporary storage available in a Colab environment, GCS can reliably store petabytes of data and is built to withstand hardware failures. This ensures that your data is safe and accessible, even for very large datasets that might exceed Colab's local storage limits or be lost if the Colab session ends unexpectedly.\n",
        "#\n",
        " 2.  **Decoupling and Access Control:** Staging data in GCS decouples the storage from the compute environment (Colab). This means that once data is in GCS, it can be accessed by various Google Cloud services (like BigQuery, Dataflow, or Vertex AI) independently of your Colab session. This allows for more robust and flexible data pipelines. Furthermore, GCS provides fine-grained access control, allowing you to manage who can read or write to your data, which is crucial for security and collaboration, something that is less manageable with local Colab files.\n"
      ],
      "metadata": {
        "id": "BzuYQwcVmkfZ"
      },
      "id": "BzuYQwcVmkfZ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cxt75SVukEdl"
      },
      "source": [
        "## 4) BigQuery dataset & loads — What & Why\n",
        "Create dataset `netflix` and load six CSVs with **autodetect** for speed (we’ll enforce schemas later)."
      ],
      "id": "cxt75SVukEdl"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_VyB5VSUkEdl"
      },
      "source": [
        "### Build Prompt (two cells)\n",
        "**Cell A:** Create (idempotently) dataset `netflix` in US multi-region; if it exists, print a friendly message.  \n",
        "**Cell B:** Load tables from `gs://$BUCKET_NAME/netflix/`:\n",
        "`users, movies, watch_history, recommendation_logs, search_logs, reviews`\n",
        "with `--skip_leading_rows=1 --autodetect --source_format=CSV`.\n",
        "Finish with row-count queries for each table.\n"
      ],
      "id": "_VyB5VSUkEdl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xHPLk9zdkEdm",
        "outputId": "227b0763-8d06-40f7-a4d7-f978c069610d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset 'mgmt-467:netflix' successfully created.\n"
          ]
        }
      ],
      "source": [
        "# prompt: Cell A: Create (idempotently) dataset netflix in US multi-region; if it exists, print a friendly message.\n",
        "DATASET=\"netflix\"\n",
        "# Attempt to create; ignore if exists\n",
        "!bq --location=US mk -d --description \"MGMT467 Netflix dataset\" $DATASET || echo \"Dataset may already exist.\""
      ],
      "id": "xHPLk9zdkEdm"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Load six CSVs (users, movies, watch_history, recommendation_logs, search_logs, reviews) from GCS\n",
        "# with autodetect and header skip.\n",
        "# Run row■count queries to confirm loads. Use big query to help achieve this properly\n",
        "\n",
        "from google.cloud import bigquery\n",
        "\n",
        "# Initialize BigQuery client\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Define the dataset and table names\n",
        "dataset_id = \"netflix\"\n",
        "tables = [\n",
        "    \"users\",\n",
        "    \"movies\",\n",
        "    \"watch_history\",\n",
        "    \"recommendation_logs\",\n",
        "    \"search_logs\",\n",
        "    \"reviews\",\n",
        "]\n",
        "\n",
        "# Construct the GCS URI for the CSV files\n",
        "gcs_uri_prefix = f\"gs://{os.environ['BUCKET_NAME']}/netflix/\"\n",
        "\n",
        "# Load each CSV file into BigQuery\n",
        "for table_name in tables:\n",
        "    table_id = f\"{dataset_id}.{table_name}\"\n",
        "    source_file = f\"{gcs_uri_prefix}{table_name}.csv\" # Assuming filenames match table names\n",
        "\n",
        "    job_config = bigquery.LoadJobConfig(\n",
        "        skip_leading_rows=1,  # Skip the header row\n",
        "        autodetect=True,      # Autodetect schema and types\n",
        "        source_format=bigquery.SourceFormat.CSV,\n",
        "    )\n",
        "\n",
        "    print(f\"Loading {source_file} into {table_id}...\")\n",
        "    load_job = client.load_table_from_uri(\n",
        "        source_file, table_id, job_config=job_config\n",
        "    )\n",
        "    load_job.result()  # Wait for the job to complete\n",
        "\n",
        "    print(f\"Loaded {load_job.output_rows} rows into {table_id}.\")\n",
        "\n",
        "# Query row counts for each table to verify\n",
        "print(\"\\nVerifying row counts:\")\n",
        "for table_name in tables:\n",
        "    table_id = f\"{dataset_id}.{table_name}\"\n",
        "    query = f\"SELECT COUNT(*) FROM `{table_id}`\"\n",
        "    query_job = client.query(query)\n",
        "    rows = query_job.result()\n",
        "    for row in rows:\n",
        "        print(f\"Table '{table_id}': {row[0]} rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROoa1xVSBBEw",
        "outputId": "de48c4e6-9db4-4a10-9818-535368ee86d8"
      },
      "id": "ROoa1xVSBBEw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading gs://mgmt467-netflix-379f8b03/netflix/users.csv into netflix.users...\n",
            "Loaded 10300 rows into netflix.users.\n",
            "Loading gs://mgmt467-netflix-379f8b03/netflix/movies.csv into netflix.movies...\n",
            "Loaded 1040 rows into netflix.movies.\n",
            "Loading gs://mgmt467-netflix-379f8b03/netflix/watch_history.csv into netflix.watch_history...\n",
            "Loaded 105000 rows into netflix.watch_history.\n",
            "Loading gs://mgmt467-netflix-379f8b03/netflix/recommendation_logs.csv into netflix.recommendation_logs...\n",
            "Loaded 52000 rows into netflix.recommendation_logs.\n",
            "Loading gs://mgmt467-netflix-379f8b03/netflix/search_logs.csv into netflix.search_logs...\n",
            "Loaded 26500 rows into netflix.search_logs.\n",
            "Loading gs://mgmt467-netflix-379f8b03/netflix/reviews.csv into netflix.reviews...\n",
            "Loaded 15450 rows into netflix.reviews.\n",
            "\n",
            "Verifying row counts:\n",
            "Table 'netflix.users': 20600 rows\n",
            "Table 'netflix.movies': 2080 rows\n",
            "Table 'netflix.watch_history': 210000 rows\n",
            "Table 'netflix.recommendation_logs': 104000 rows\n",
            "Table 'netflix.search_logs': 53000 rows\n",
            "Table 'netflix.reviews': 30900 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VCDn53jVkEdm"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single query that returns `table_name, row_count` for all six tables in `${GOOGLE_CLOUD_PROJECT}.netflix`.\n"
      ],
      "id": "VCDn53jVkEdm"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\nVerifying row counts:\")\n",
        "for table_name in tables:\n",
        "    table_id = f\"{dataset_id}.{table_name}\"\n",
        "    query = f\"SELECT COUNT(*) FROM `{table_id}`\"\n",
        "    query_job = client.query(query)\n",
        "    rows = query_job.result()\n",
        "    for row in rows:\n",
        "        print(f\"Table '{table_id}': {row[0]} rows\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4xB2J8pgEehC",
        "outputId": "abf0f966-729e-46a3-e1a1-2c6c3e761a2a"
      },
      "id": "4xB2J8pgEehC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Verifying row counts:\n",
            "Table 'netflix.users': 20600 rows\n",
            "Table 'netflix.movies': 2080 rows\n",
            "Table 'netflix.watch_history': 210000 rows\n",
            "Table 'netflix.recommendation_logs': 104000 rows\n",
            "Table 'netflix.search_logs': 53000 rows\n",
            "Table 'netflix.reviews': 30900 rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qGfNYcEzkEdm"
      },
      "source": [
        "**Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?"
      ],
      "id": "qGfNYcEzkEdm"
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Reflection:** When is `autodetect` acceptable? When should you enforce explicit schemas and why?\n",
        " `autodetect` is acceptable for:\n",
        " 1.  **Exploratory Data Analysis (EDA):** When you're quickly exploring a new dataset and don't yet know its structure or data types. It allows for rapid ingestion and initial analysis.\n",
        " 2.  **Small, Trusted Datasets:** For small, well-understood datasets where you are confident in the data's consistency and format, `autodetect` can save time.\n",
        " 3.  **Prototyping:** During the initial stages of building a pipeline, `autodetect` can be useful to get data into BigQuery quickly for testing.\n",
        "#\n",
        "You should enforce explicit schemas when:\n",
        " 1.  **Production Pipelines:** In production environments, explicit schemas are crucial for data quality, consistency, and reliability. `autodetect` can be brittle and might infer incorrect data types (e.g., treating a numeric ID as an integer when it should be a string, or misinterpreting dates).\n",
        " 2.  **Data Governance and Compliance:** Many organizations have strict data governance policies that require defined schemas for data integrity, security, and compliance (e.g., GDPR, HIPAA).\n",
        " 3.  **Performance Optimization:** Explicitly defining data types and partitioning/clustering can significantly improve query performance and reduce costs in BigQuery. `autodetect` might not choose the optimal configurations.\n",
        " 4.  **Preventing Data Corruption:** If a CSV file contains unexpected values or formatting, `autodetect` might fail the load job or, worse, infer incorrect types that lead to data corruption or incorrect analysis downstream. An explicit schema provides a clear contract for the data.\n",
        " 5.  **Schema Evolution Management:** When schemas need to change, having an explicit schema allows for controlled evolution (e.g., adding new nullable columns) rather than relying on `autodetect` to guess the changes, which could lead to unexpected behavior.\n",
        "#\n",
        " **Why enforce explicit schemas?**\n",
        " Enforcing explicit schemas provides:\n",
        " -   **Data Quality Assurance:** Ensures data conforms to expected types and formats, preventing errors and inconsistencies.\n",
        " -   **Predictability:** Guarantees that data is loaded as intended, making downstream processes more reliable.\n",
        " -   **Performance:** Allows for optimization through correct data type selection, partitioning, and clustering.\n",
        " -   **Cost Control:** Prevents unexpected costs due to inefficient data types or large amounts of scanned data.\n",
        " -   **Security and Compliance:** Helps meet regulatory requirements by ensuring data is handled appropriately."
      ],
      "metadata": {
        "id": "wspCzsqFEq6B"
      },
      "id": "wspCzsqFEq6B"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eCZffE1skEdm"
      },
      "source": [
        "## 5) Data Quality (DQ) — Concepts we care about\n",
        "- **Missingness** (MCAR/MAR/MNAR). Impute vs drop. Add `is_missing_*` indicators.\n",
        "- **Duplicates** (exact vs near). Double-counted engagement corrupts labels & KPIs.\n",
        "- **Outliers** (IQR). Winsorize/cap vs robust models. Always **flag** and explain.\n",
        "- **Reproducibility**. Prefer `CREATE OR REPLACE` and deterministic keys.\n"
      ],
      "id": "eCZffE1skEdm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_WWhCWRkEdm"
      },
      "source": [
        "### 5.1 Missingness (users) — What & Why\n",
        "Measure % missing and check if missingness depends on another variable (MAR) → potential bias & instability."
      ],
      "id": "B_WWhCWRkEdm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMGDK9bikEdm"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Total rows and % missing in `region`, `plan_tier`, `age_band` from `users`.\n",
        "2) `% plan_tier missing by region` ordered descending. Add comments on MAR.\n"
      ],
      "id": "JMGDK9bikEdm"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JF_WpDfdkEdm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abade24c-8c7d-42e9-9695-9cee2e97ec7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calculating missingness in users table...\n",
            "Total Rows: 20600\n",
            "Percent Missing Country: 0.00%\n",
            "Percent Missing Subscription Plan: 0.00%\n",
            "Percent Missing Age: 11.93%\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Missingness in users table\n",
        "# Calculate total rows and percentage of missing values for specified columns.\n",
        "# This helps identify columns with significant missing data.\n",
        "print(\"Calculating missingness in users table...\")\n",
        "\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client()\n",
        "\n",
        "# Define the SQL query as a Python multi-line string\n",
        "missingness_query = f\"\"\"\n",
        "SELECT\n",
        "    COUNT(*) AS total_rows,\n",
        "    SUM(CASE WHEN country IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_missing_country,\n",
        "    SUM(CASE WHEN subscription_plan IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_missing_subscription_plan,\n",
        "    SUM(CASE WHEN age IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_missing_age\n",
        "FROM\n",
        "    `{PROJECT_ID}.netflix.users`\n",
        "\"\"\"\n",
        "\n",
        "# Execute the query using the BigQuery client library\n",
        "query_job = client.query(missingness_query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(f\"Total Rows: {row.total_rows}\")\n",
        "    print(f\"Percent Missing Country: {row.percent_missing_country:.2f}%\")\n",
        "    print(f\"Percent Missing Subscription Plan: {row.percent_missing_subscription_plan:.2f}%\")\n",
        "    print(f\"Percent Missing Age: {row.percent_missing_age:.2f}%\")"
      ],
      "id": "JF_WpDfdkEdm"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T51Bfk_TkEdn"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n"
      ],
      "id": "T51Bfk_TkEdn"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a query that prints the three missingness percentages from (1), rounded to two decimals.\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "-- Users: % missing per column\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT\n",
        "       ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(f\"Percent Missing Country: {row.pct_missing_country}%\")\n",
        "    print(f\"Percent Missing Subscription Plan: {row.pct_missing_subscription_plan}%\")\n",
        "    print(f\"Percent Missing Age: {row.pct_missing_age}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLbUYqmMKvvm",
        "outputId": "b1445f35-f326-4649-9de4-9ca0dc8d8533"
      },
      "id": "GLbUYqmMKvvm",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Percent Missing Country: 0.0%\n",
            "Percent Missing Subscription Plan: 0.0%\n",
            "Percent Missing Age: 11.93%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7gHKncw1kEdn"
      },
      "source": [
        "**Reflection:** Which columns are most missing? Hypothesize MCAR/MAR/MNAR and why."
      ],
      "id": "7gHKncw1kEdn"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the previous output, the `age` column has the highest percentage of missing values.\n",
        "\n",
        "Here's a hypothesis for each column:\n",
        "\n",
        "*   **`country`**: If the missingness of `country` is independent of other variables in the dataset (e.g., users who don't provide their country are otherwise similar to those who do), it could be **MCAR (Missing Completely At Random)**. However, it's more likely that users in certain regions might be less inclined to share their country, or there might be technical issues in data collection for specific regions, suggesting **MAR (Missing At Random)** or even **MNAR (Missing Not At Random)** if the reason for not providing the country is related to unobserved user characteristics.\n",
        "\n",
        "*   **`subscription_plan`**: If the missingness of `plan_tier` is related to other observed variables (e.g., users with lower engagement or specific demographics are less likely to have a plan tier recorded), it would be **MAR**. For instance, if users who are on a free trial or have recently churned are less likely to have their `plan_tier` recorded, and these factors are observable elsewhere, it's MAR. If the reason for not having a `plan_tier` recorded is inherently tied to the user's behavior or status in a way that isn't captured by other variables (e.g., they are simply not a paying customer and this status isn't explicitly recorded), it could be **MNAR**.\n",
        "\n",
        "*   **`age`**: The `age` column often has a high percentage of missing values. This could be **MCAR** if users simply choose not to provide their age, and this choice is unrelated to other factors. However, it's more likely to be **MAR** or **MNAR**. For example, younger users might be more hesitant to share their age than older users, or users who are not interested in age-specific content might not provide it. If the missingness is related to other observable user attributes (like their subscription plan or viewing habits), it's MAR. If the reason for not providing age is intrinsic to the user's profile and not captured by other data points, it's MNAR.\n",
        "\n",
        "Without further analysis of correlations between missingness and other variables, it's difficult to definitively classify. However, `age` and `subscription_plan` are strong candidates for **MAR** due to the likelihood that their missingness is related to other observable user characteristics or behaviors.\n"
      ],
      "metadata": {
        "id": "omW2dudcL6Ab"
      },
      "id": "omW2dudcL6Ab"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok48gXiokEdn"
      },
      "source": [
        "### 5.2 Duplicates (watch_history) — What & Why\n",
        "Find exact duplicate interaction records and keep **one best** per group (deterministic policy)."
      ],
      "id": "ok48gXiokEdn"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-2CrzekkEdn"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Report duplicate groups on `(user_id, movie_id, event_ts, device_type)` with counts (top 20).\n",
        "2) Create table `watch_history_dedup` that keeps one row per group (prefer higher `progress_ratio`, then `minutes_watched`). Add comments.\n"
      ],
      "id": "t-2CrzekkEdn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VF9WOeeNkEdn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862ccf11-82b9-43d9-8976-fb4647b32fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Row(('user_03310', 'movie_0640', datetime.date(2024, 9, 8), 'Smart TV', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00391', 'movie_0893', datetime.date(2024, 8, 26), 'Laptop', 8), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03043', 'movie_0465', datetime.date(2024, 2, 3), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_00965', 'movie_0991', datetime.date(2024, 2, 14), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08681', 'movie_0332', datetime.date(2024, 6, 13), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03021', 'movie_0602', datetime.date(2025, 2, 23), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01469', 'movie_0237', datetime.date(2025, 1, 17), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_05952', 'movie_0893', datetime.date(2024, 4, 29), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06295', 'movie_0097', datetime.date(2025, 2, 24), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_03408', 'movie_0146', datetime.date(2025, 6, 2), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08826', 'movie_0133', datetime.date(2025, 4, 11), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_05811', 'movie_0177', datetime.date(2024, 5, 7), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02976', 'movie_0987', datetime.date(2024, 9, 19), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02950', 'movie_0928', datetime.date(2025, 6, 3), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02126', 'movie_0642', datetime.date(2025, 2, 9), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_07617', 'movie_0785', datetime.date(2024, 7, 14), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_02359', 'movie_0108', datetime.date(2024, 10, 12), 'Desktop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_01143', 'movie_0166', datetime.date(2024, 5, 28), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_06462', 'movie_0588', datetime.date(2025, 2, 10), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n",
            "Row(('user_08177', 'movie_0593', datetime.date(2025, 1, 15), 'Laptop', 6), {'user_id': 0, 'movie_id': 1, 'watch_date': 2, 'device_type': 3, 'dup_count': 4})\n"
          ]
        }
      ],
      "source": [
        "# prompt: Report duplicate groups on (user_id, movie_id, event_ts, device_type) with counts (top 20).\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(row)"
      ],
      "id": "VF9WOeeNkEdn"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5yVMydWkEdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9af2c0-46ba-4706-c867-c7474108732d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deduplicated table watch_history_dedup created successfully.\n"
          ]
        }
      ],
      "source": [
        "# prompt: Create table watch_history_dedup that keeps one row per group (prefer higher progress_ratio, then minutes_watched). Add comments.\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS rk\n",
        "  FROM `{project_id}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "print(\"Deduplicated table watch_history_dedup created successfully.\")"
      ],
      "id": "T5yVMydWkEdo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ok4mdzTvkEdo"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a before/after count query comparing raw vs `watch_history_dedup`.\n"
      ],
      "id": "ok4mdzTvkEdo"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a before/after count query comparing raw vs watch_history_dedup.\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  (SELECT COUNT(*) FROM `{project_id}.netflix.watch_history`) AS raw_count,\n",
        "  (SELECT COUNT(*) FROM `{project_id}.netflix.watch_history_dedup`) AS dedup_count;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(f\"Raw Count: {row.raw_count}, Deduplicated Count: {row.dedup_count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "46OpMrfNM_yP",
        "outputId": "e7de6c9e-fd66-4316-a1ae-0ead42e6d866"
      },
      "id": "46OpMrfNM_yP",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Raw Count: 210000, Deduplicated Count: 100000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYZufwZkEdo"
      },
      "source": [
        "**Reflection:** Why do duplicates arise (natural vs system-generated)? How do they corrupt labels and KPIs?"
      ],
      "id": "9EYZufwZkEdo"
    },
    {
      "cell_type": "markdown",
      "source": [
        " **Why duplicates arise:**\n",
        "#\n",
        " *   **Natural Duplicates:** These can occur due to user behavior. For example, a user might accidentally click a \"play\" button twice, or a network glitch might cause a single interaction to be sent to the server multiple times. In some systems, a single logical event (like watching a movie) might be recorded as multiple distinct events if it spans across different sessions or involves multiple micro-interactions (e.g., buffering events, seeking events).\n",
        "#\n",
        " *   **System-Generated Duplicates:** These are often a result of technical issues or design flaws in the data ingestion or processing pipeline:\n",
        "     *   **Retries:** If a data ingestion service fails to receive an acknowledgment for a record, it might retry sending the same record, leading to duplicates if the initial send was actually successful.\n",
        "     *   **Batch Processing Errors:** When data is processed in batches, errors in the batch job logic or failures during the commit phase can result in partial or duplicate writes.\n",
        "     *   **Distributed Systems:** In distributed systems, coordinating writes across multiple nodes can be complex. Race conditions or inconsistencies in distributed transactions can lead to duplicate entries.\n",
        "     *   **Data Merging:** When combining data from different sources, if the sources have overlapping records and the merging process isn't designed to handle duplicates, they can be introduced.\n",
        "#\n",
        " **How duplicates corrupt labels and KPIs:**\n",
        "#\n",
        " Duplicates can significantly distort metrics and labels, leading to incorrect insights and flawed decision-making.\n",
        "#\n",
        " *   **Corrupted Labels:**\n",
        "     *   **Engagement Metrics:** If a user watches a movie twice (or the system records it twice), metrics like \"total watch time\" or \"number of movies watched\" will be inflated. This can lead to misinterpreting user engagement levels.\n",
        "     *   **Conversion Rates:** If a duplicate interaction is counted as a separate event, it can artificially inflate conversion rates or other funnel metrics. For example, if a \"sign-up\" event is duplicated, the number of sign-ups would appear higher than it actually is.\n",
        "     *   **User Segmentation:** If duplicate interactions are used to define user segments (e.g., \"highly engaged users\"), these segments might be based on inflated activity, leading to inaccurate profiling.\n",
        "#\n",
        " *   **Corrupted KPIs (Key Performance Indicators):**\n",
        "     *   **Revenue and Monetization:** KPIs related to revenue (e.g., average revenue per user, subscription renewals) can be skewed if duplicate transactions or engagement events are counted.\n",
        "     *   **User Retention:** If duplicate \"active user\" events are counted, retention rates might appear higher than they are, masking potential churn issues.\n",
        "     *   **Feature Usage:** KPIs measuring the usage of specific features can be inflated, leading to incorrect assessments of feature adoption and success.\n",
        "     *   **Performance Benchmarking:** When comparing performance over time or against benchmarks, duplicates can make current performance look better or worse than it truly is, hindering accurate performance evaluation.\n",
        "#\n",
        " In essence, duplicates introduce noise into the data, making it unreliable for analysis. This can lead to over- or under-estimation of user behavior, incorrect resource allocation, and flawed strategic decisions based on misleading metrics. Therefore, identifying and handling duplicates is a critical step in data quality assurance."
      ],
      "metadata": {
        "id": "5hcOtSOuOQ_m"
      },
      "id": "5hcOtSOuOQ_m"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KLuWW055kEdo"
      },
      "source": [
        "### 5.3 Outliers (minutes_watched) — What & Why\n",
        "Estimate extreme values via IQR; report % outliers; **winsorize** to P01/P99 for robustness while also **flagging** extremes."
      ],
      "id": "KLuWW055kEdo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBsKy154kEdo"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **two BigQuery SQL cells**:\n",
        "1) Compute IQR bounds for `minutes_watched` on `watch_history_dedup` and report % outliers.\n",
        "2) Create `watch_history_robust` with `minutes_watched_capped` capped at P01/P99; return quantile summaries before/after.\n"
      ],
      "id": "CBsKy154kEdo"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Compute IQR bounds for watch_duration_minutes on watch_history_dedup and report % outliers.\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH\n",
        "  quantiles AS (\n",
        "    SELECT\n",
        "      APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "      APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "    FROM\n",
        "      `{project_id}.netflix.watch_history_dedup`\n",
        "  ),\n",
        "  bounds AS (\n",
        "    SELECT\n",
        "      q1,\n",
        "      q3,\n",
        "      q1 - 1.5 * (q3 - q1) AS lower_bound,\n",
        "      q3 + 1.5 * (q3 - q1) AS upper_bound\n",
        "    FROM\n",
        "      quantiles\n",
        "  )\n",
        "SELECT\n",
        "  COUNT(*) AS total_rows,\n",
        "  SUM(CASE WHEN watch_duration_minutes < b.lower_bound OR watch_duration_minutes > b.upper_bound THEN 1 ELSE 0 END) AS outlier_count,\n",
        "  SUM(CASE WHEN watch_duration_minutes < b.lower_bound OR watch_duration_minutes > b.upper_bound THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS outlier_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_dedup`, bounds b;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(f\"Total Rows: {row.total_rows}\")\n",
        "    print(f\"Outlier Count: {row.outlier_count}\")\n",
        "    print(f\"Outlier Percentage: {row.outlier_percentage:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O3qTkIHWOmT3",
        "outputId": "629b6c27-7596-4ed8-a30c-33a3336a6529"
      },
      "id": "O3qTkIHWOmT3",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Rows: 100000\n",
            "Outlier Count: 3508\n",
            "Outlier Percentage: 3.51%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ycjNSNvukEdo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e81348b-4a56-403d-de76-8d4d7db87aa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Quantile P1 (before capping): 4.4\n",
            "Quantile P99 (before capping): 366.0\n",
            "\n",
            "Creating watch_history_robust table with capped minutes_watched...\n",
            "Table watch_history_robust created successfully.\n",
            "\n",
            "Quantile summaries AFTER capping minutes_watched:\n",
            "P1 Capped (after capping): 4.5\n",
            "P99 Capped (after capping): 358.1\n"
          ]
        }
      ],
      "source": [
        "# prompt: Create watch_history_robust with minutes_watched_capped capped at P01/P99; return quantile summaries before/after.\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "# Get the P01 and P99 values from the original deduped table first\n",
        "quantile_values_query = f\"\"\"\n",
        "SELECT\n",
        "  APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(1)] AS p01,\n",
        "  APPROX_QUANTILES(watch_duration_minutes, 100)[OFFSET(99)] AS p99\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_dedup`\n",
        "\"\"\"\n",
        "query_job_quantiles = client.query(quantile_values_query)\n",
        "quantile_results = query_job_quantiles.result()\n",
        "\n",
        "p01_val = None\n",
        "p99_val = None\n",
        "for row in quantile_results:\n",
        "    p01_val = row.p01\n",
        "    p99_val = row.p99\n",
        "    print(f\"Quantile P1 (before capping): {p01_val}\")\n",
        "    print(f\"Quantile P99 (before capping): {p99_val}\")\n",
        "\n",
        "# Now, create the robust table with capped minutes_watched\n",
        "print(\"\\nCreating watch_history_robust table with capped minutes_watched...\")\n",
        "query_create_robust = f\"\"\"\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_robust` AS\n",
        "SELECT\n",
        "  *, -- Select all original columns\n",
        "  -- Cap watch_duration_minutes at P01 and P99 values obtained above\n",
        "  CASE\n",
        "    WHEN watch_duration_minutes < {p01_val} THEN {p01_val}\n",
        "    WHEN watch_duration_minutes > {p99_val} THEN {p99_val}\n",
        "    ELSE watch_duration_minutes\n",
        "  END AS watch_duration_minutes_capped\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_dedup`;\n",
        "\"\"\"\n",
        "\n",
        "query_job_create = client.query(query_create_robust)\n",
        "query_job_create.result() # Wait for the job to complete\n",
        "print(\"Table watch_history_robust created successfully.\")\n",
        "\n",
        "# Finally, get the quantile summaries after capping\n",
        "print(\"\\nQuantile summaries AFTER capping minutes_watched:\")\n",
        "query_after = f\"\"\"\n",
        "SELECT\n",
        "  APPROX_QUANTILES(watch_duration_minutes_capped, 100)[OFFSET(1)] AS p1_capped,\n",
        "  APPROX_QUANTILES(watch_duration_minutes_capped, 100)[OFFSET(99)] AS p99_capped\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_robust`\n",
        "\"\"\"\n",
        "query_job_after = client.query(query_after)\n",
        "results_after = query_job_after.result()\n",
        "for row in results_after:\n",
        "    print(f\"P1 Capped (after capping): {row.p1_capped}\")\n",
        "    print(f\"P99 Capped (after capping): {row.p99_capped}\")\n"
      ],
      "id": "ycjNSNvukEdo"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOPqkHGlkEdo"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a query that shows min/median/max before vs after capping.\n"
      ],
      "id": "pOPqkHGlkEdo"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a query that shows min/median/max before vs after capping.\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "client = bigquery.Client(project=project_id)\n",
        "\n",
        "query = f\"\"\"\n",
        "SELECT\n",
        "  'Before Capping' AS stage,\n",
        "  MIN(watch_duration_minutes) AS min_minutes,\n",
        "  APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_minutes,\n",
        "  MAX(watch_duration_minutes) AS max_minutes\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_dedup`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "  'After Capping' AS stage,\n",
        "  MIN(watch_duration_minutes_capped) AS min_minutes,\n",
        "  APPROX_QUANTILES(watch_duration_minutes_capped, 2)[OFFSET(1)] AS median_minutes,\n",
        "  MAX(watch_duration_minutes_capped) AS max_minutes\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_robust`;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(f\"Stage: {row.stage}, Min Minutes: {row.min_minutes}, Median Minutes: {row.median_minutes}, Max Minutes: {row.max_minutes}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EqUkBqZZPoLw",
        "outputId": "6e1be42e-cabf-4726-d548-faf0810263a0"
      },
      "id": "EqUkBqZZPoLw",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Stage: Before Capping, Min Minutes: 0.2, Median Minutes: 51.2, Max Minutes: 799.3\n",
            "Stage: After Capping, Min Minutes: 4.4, Median Minutes: 51.4, Max Minutes: 366.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BftLlQgpkEdo"
      },
      "source": [
        "**Reflection:** When might capping be harmful? Name a model type less sensitive to outliers and why."
      ],
      "id": "BftLlQgpkEdo"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**When capping might be harmful:**\n",
        "#\n",
        " Capping (winsorizing) can be harmful if the extreme values, while rare, contain genuinely important information or represent a distinct, valid phenomenon that you don't want to obscure. For example:\n",
        "#\n",
        " 1.  **Fraud Detection:** In fraud detection, extreme values (e.g., unusually large transaction amounts) are often the very signals you are trying to detect. Capping these values would remove the evidence of fraud, making the model less effective at identifying fraudulent activities.\n",
        " 2.  **Rare but Significant Events:** If extreme values represent rare but critical events (e.g., a sudden surge in demand for a product, a major system failure), capping them might mask these important signals, leading to a failure to recognize or respond to these events.\n",
        " 3.  **Misinterpretation of Data Distribution:** If the extreme values are not truly outliers but represent the upper or lower bounds of a legitimate, albeit skewed, distribution, capping them can distort the true nature of the data and lead to incorrect conclusions about the population.\n",
        " 4.  **Loss of Information for Specific Analyses:** If the goal is to understand the full range of observed behavior, including the extremes, capping removes this information. For instance, if you're analyzing the impact of very long viewing sessions on user satisfaction, capping these sessions would prevent you from studying that specific relationship.\n",
        "#\n",
        " **Model type less sensitive to outliers and why:**\n",
        "#\n",
        " **Tree-based models**, such as Decision Trees, Random Forests, and Gradient Boosting Machines (like XGBoost or LightGBM), are generally less sensitive to outliers compared to models that rely on distance calculations or assume a specific data distribution (like linear regression or SVMs).\n",
        "#\n",
        " **Why they are less sensitive:**\n",
        "#\n",
        " *   **Splitting Mechanism:** These models work by recursively partitioning the data based on feature values. A split is determined by finding a threshold that best separates the data according to the target variable. Outliers, being extreme values, typically fall into a single partition. While they might influence the exact position of a split, they don't disproportionately affect the overall structure of the tree as much as they would in models that use means, variances, or distances.\n",
        " *   **No Assumption of Linearity or Distribution:** Unlike linear models that assume a linear relationship between features and the target and are heavily influenced by the mean and variance, tree-based models make no such assumptions. They can capture complex, non-linear relationships and are robust to variations in data distribution.\n",
        " *   **Focus on Relative Order:** The splits in a tree are based on the relative order of data points, not their absolute magnitude. An outlier might be far from the other data points, but its position relative to other points within its partition doesn't drastically alter the splitting criteria for other parts of the tree.\n",
        "#\n",
        " While tree-based models are more robust, extremely influential outliers can still have some impact. However, compared to models like linear regression, they are significantly less affected."
      ],
      "metadata": {
        "id": "1rU3pfS8P9cr"
      },
      "id": "1rU3pfS8P9cr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVrvKV4ekEdp"
      },
      "source": [
        "### 5.4 Business anomaly flags — What & Why\n",
        "Human-readable flags help both product decisioning and ML features (e.g., binge behavior)."
      ],
      "id": "PVrvKV4ekEdp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjum5O6hkEdp"
      },
      "source": [
        "### Build Prompt\n",
        "Generate **three BigQuery SQL cells** (adjust if columns differ):\n",
        "1) In `watch_history_robust`, compute and summarize `flag_binge` for sessions > 8 hours.\n",
        "2) In `users`, compute and summarize `flag_age_extreme` if age can be parsed from `age_band` (<10 or >100).\n",
        "3) In `movies`, compute and summarize `flag_duration_anomaly` where `duration_min` < 15 or > 480 (if exists).\n",
        "Each cell should output count and percentage and include 1–2 comments.\n"
      ],
      "id": "vjum5O6hkEdp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uY7YkCbkEdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b94b5fe9-9e7e-4db7-d714-d6f0e84dc544"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing flag_binge for watch_history_robust...\n",
            "Total Sessions: 100000\n",
            "Binge Sessions Count (>8 hours): 0\n",
            "Binge Sessions Percentage: 0.00%\n",
            "\n",
            "Summarizing flag_age_extreme for users...\n",
            "Total Users (with age data): 18142\n",
            "Extreme Age Count (<10 or >100): 358\n",
            "Extreme Age Percentage: 1.97%\n",
            "\n",
            "Summarizing flag_duration_anomaly for movies...\n",
            "Total Movies (with duration data): 2080\n",
            "Duration Anomaly Count (<15 or >480 minutes): 46\n",
            "Duration Anomaly Percentage: 2.21%\n"
          ]
        }
      ],
      "source": [
        "# prompt: In watch_history_robust, compute and summarize flag_binge for sessions > 8 hours.\n",
        "\n",
        "# Note: This cell initially tried to flag extreme ages from 'age_band', which does not exist.\n",
        "# Correcting to flag extreme ages directly from the 'age' column in the users table.\n",
        "# This helps identify potential data entry errors or unusual age distributions.\n",
        "\n",
        "# Re-initializing BigQuery client for clarity in this cell\n",
        "from google.cloud import bigquery\n",
        "client = bigquery.Client()\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "print(\"Summarizing flag_binge for watch_history_robust...\")\n",
        "query_binge = f\"\"\"\n",
        "SELECT\n",
        "  COUNT(*) AS total_sessions,\n",
        "  SUM(CASE WHEN watch_duration_minutes_capped > (8 * 60) THEN 1 ELSE 0 END) AS binge_sessions_count,\n",
        "  SUM(CASE WHEN watch_duration_minutes_capped > (8 * 60) THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS binge_sessions_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_robust`\n",
        "\"\"\"\n",
        "\n",
        "query_job_binge = client.query(query_binge)\n",
        "results_binge = query_job_binge.result()\n",
        "\n",
        "for row in results_binge:\n",
        "    print(f\"Total Sessions: {row.total_sessions}\")\n",
        "    print(f\"Binge Sessions Count (>8 hours): {row.binge_sessions_count}\")\n",
        "    print(f\"Binge Sessions Percentage: {row.binge_sessions_percentage:.2f}%\")\n",
        "\n",
        "print(\"\\nSummarizing flag_age_extreme for users...\")\n",
        "# Compute and summarize flag_age_extreme if age is <10 or >100\n",
        "query_age_extreme = f\"\"\"\n",
        "SELECT\n",
        "  COUNT(*) AS total_users,\n",
        "  SUM(CASE WHEN age < 10 OR age > 100 THEN 1 ELSE 0 END) AS extreme_age_count,\n",
        "  SUM(CASE WHEN age < 10 OR age > 100 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS extreme_age_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.users`\n",
        "WHERE age IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "query_job_age_extreme = client.query(query_age_extreme)\n",
        "results_age_extreme = query_job_age_extreme.result()\n",
        "\n",
        "for row in results_age_extreme:\n",
        "    print(f\"Total Users (with age data): {row.total_users}\")\n",
        "    print(f\"Extreme Age Count (<10 or >100): {row.extreme_age_count}\")\n",
        "    print(f\"Extreme Age Percentage: {row.extreme_age_percentage:.2f}%\")\n",
        "\n",
        "print(\"\\nSummarizing flag_duration_anomaly for movies...\")\n",
        "# Compute and summarize flag_duration_anomaly where duration_min < 15 or > 480\n",
        "query_duration_anomaly = f\"\"\"\n",
        "SELECT\n",
        "  COUNT(*) AS total_movies,\n",
        "  SUM(CASE WHEN duration_minutes < 15 OR duration_minutes > 480 THEN 1 ELSE 0 END) AS duration_anomaly_count,\n",
        "  SUM(CASE WHEN duration_minutes < 15 OR duration_minutes > 480 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS duration_anomaly_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.movies`\n",
        "WHERE duration_minutes IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "query_job_duration_anomaly = client.query(query_duration_anomaly)\n",
        "results_duration_anomaly = query_job_duration_anomaly.result()\n",
        "\n",
        "for row in results_duration_anomaly:\n",
        "    print(f\"Total Movies (with duration data): {row.total_movies}\")\n",
        "    print(f\"Duration Anomaly Count (<15 or >480 minutes): {row.duration_anomaly_count}\")\n",
        "    print(f\"Duration Anomaly Percentage: {row.duration_anomaly_percentage:.2f}%\")\n"
      ],
      "id": "_uY7YkCbkEdp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oeExa90MkEdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "79ad5ebf-818f-4abd-de5c-461e1ecbf5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarizing flag_age_extreme for users...\n",
            "Total Users (with age data): 18142\n",
            "Extreme Age Count (<10 or >100): 358\n",
            "Extreme Age Percentage: 1.97%\n"
          ]
        }
      ],
      "source": [
        "# prompt: In users, compute and summarize flag_age_extreme if age can be parsed from age_band (<10 or >100).\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client()\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "print(\"Summarizing flag_age_extreme for users...\")\n",
        "# Compute and summarize flag_age_extreme if age is <10 or >100\n",
        "query_age_extreme = f\"\"\"\n",
        "SELECT\n",
        "  COUNT(*) AS total_users,\n",
        "  SUM(CASE WHEN age < 10 OR age > 100 THEN 1 ELSE 0 END) AS extreme_age_count,\n",
        "  SUM(CASE WHEN age < 10 OR age > 100 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS extreme_age_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.users`\n",
        "WHERE age IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "query_job_age_extreme = client.query(query_age_extreme)\n",
        "results_age_extreme = query_job_age_extreme.result()\n",
        "\n",
        "for row in results_age_extreme:\n",
        "    print(f\"Total Users (with age data): {row.total_users}\")\n",
        "    print(f\"Extreme Age Count (<10 or >100): {row.extreme_age_count}\")\n",
        "    print(f\"Extreme Age Percentage: {row.extreme_age_percentage:.2f}%\")\n"
      ],
      "id": "oeExa90MkEdp"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxIxwQ1ikEdp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "078e883a-8a94-48ea-e007-7e7ebc9f73de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Summarizing flag_duration_anomaly for movies...\n",
            "Total Movies (with duration data): 2080\n",
            "Duration Anomaly Count (<15 or >480 minutes): 46\n",
            "Duration Anomaly Percentage: 2.21%\n"
          ]
        }
      ],
      "source": [
        "# prompt: In movies, compute and summarize flag_duration_anomaly where duration_min < 15 or > 480 (if exists). Each cell should output count and percentage and include 1–2 comments.\n",
        "\n",
        "print(\"\\nSummarizing flag_duration_anomaly for movies...\")\n",
        "# Compute and summarize flag_duration_anomaly where duration_min < 15 or > 480\n",
        "query_duration_anomaly = f\"\"\"\n",
        "SELECT\n",
        "  COUNT(*) AS total_movies,\n",
        "  SUM(CASE WHEN duration_minutes < 15 OR duration_minutes > 480 THEN 1 ELSE 0 END) AS duration_anomaly_count,\n",
        "  SUM(CASE WHEN duration_minutes < 15 OR duration_minutes > 480 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS duration_anomaly_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.movies`\n",
        "WHERE duration_minutes IS NOT NULL\n",
        "\"\"\"\n",
        "\n",
        "query_job_duration_anomaly = client.query(query_duration_anomaly)\n",
        "results_duration_anomaly = query_job_duration_anomaly.result()\n",
        "\n",
        "for row in results_duration_anomaly:\n",
        "    print(f\"Total Movies (with duration data): {row.total_movies}\")\n",
        "    print(f\"Duration Anomaly Count (<15 or >480 minutes): {row.duration_anomaly_count}\")\n",
        "    print(f\"Duration Anomaly Percentage: {row.duration_anomaly_percentage:.2f}%\")\n"
      ],
      "id": "vxIxwQ1ikEdp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HF1VV7LWkEdp"
      },
      "source": [
        "### Verification Prompt\n",
        "Generate a single compact summary query that returns two columns per flag: `flag_name, pct_of_rows`.\n"
      ],
      "id": "HF1VV7LWkEdp"
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Generate a single compact summary query that returns two columns per flag: flag_name, pct_of_rows.\n",
        "\n",
        "import os\n",
        "from google.cloud import bigquery\n",
        "\n",
        "client = bigquery.Client()\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "query = f\"\"\"\n",
        "WITH\n",
        "  binge_flags AS (\n",
        "    SELECT\n",
        "      'flag_binge' AS flag_name,\n",
        "      SUM(CASE WHEN watch_duration_minutes_capped > (8 * 60) THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS pct_of_rows\n",
        "    FROM\n",
        "      `{project_id}.netflix.watch_history_robust`\n",
        "  ),\n",
        "  age_flags AS (\n",
        "    SELECT\n",
        "      'flag_age_extreme' AS flag_name,\n",
        "      SUM(CASE WHEN age < 10 OR age > 100 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS pct_of_rows\n",
        "    FROM\n",
        "      `{project_id}.netflix.users`\n",
        "    WHERE age IS NOT NULL\n",
        "  ),\n",
        "  duration_flags AS (\n",
        "    SELECT\n",
        "      'flag_duration_anomaly' AS flag_name,\n",
        "      SUM(CASE WHEN duration_minutes < 15 OR duration_minutes > 480 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS pct_of_rows\n",
        "    FROM\n",
        "      `{project_id}.netflix.movies`\n",
        "    WHERE duration_minutes IS NOT NULL\n",
        "  )\n",
        "SELECT * FROM binge_flags\n",
        "UNION ALL\n",
        "SELECT * FROM age_flags\n",
        "UNION ALL\n",
        "SELECT * FROM duration_flags;\n",
        "\"\"\"\n",
        "\n",
        "query_job = client.query(query)\n",
        "results = query_job.result()\n",
        "\n",
        "# Print the results\n",
        "for row in results:\n",
        "    print(f\"Flag Name: {row.flag_name}, Percentage of Rows: {row.pct_of_rows:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pdtcP6YqVTjV",
        "outputId": "8bfe2c70-632b-47f3-9e2f-19b3df368c19"
      },
      "id": "pdtcP6YqVTjV",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Flag Name: flag_binge, Percentage of Rows: 0.00%\n",
            "Flag Name: flag_age_extreme, Percentage of Rows: 1.97%\n",
            "Flag Name: flag_duration_anomaly, Percentage of Rows: 2.21%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ymjaNO2ykEdp"
      },
      "source": [
        "**Reflection:** Which anomaly flag is most common? Which would you keep as a feature and why?"
      ],
      "id": "ymjaNO2ykEdp"
    },
    {
      "cell_type": "markdown",
      "source": [
        " To determine which anomaly flag is most common, we need to look at the percentages calculated in the previous verification prompt. Assuming the output showed:\n",
        "#\n",
        " *   `flag_binge`: X%\n",
        " *   `flag_age_extreme`: Y%\n",
        " *   `flag_duration_anomaly`: Z%\n",
        "#\n",
        " We would identify the flag with the highest percentage as the most common.\n",
        "#\n",
        " **Which flag to keep as a feature and why:**\n",
        "#\n",
        " The choice of which flag to keep as a feature depends heavily on the downstream modeling task and the business objective. However, here's a rationale for each:\n",
        "#\n",
        " 1.  **`flag_binge`**:\n",
        "     *   **Why keep:** This flag indicates users who exhibit a specific, potentially high-engagement behavior (watching for over 8 hours). This could be a strong indicator of user loyalty, addiction to content, or a specific viewing pattern that might be predictive of subscription renewal, churn (if binge-watching leads to burnout), or interest in specific types of long-form content.\n",
        "     *   **Use case:** Useful for predicting subscription retention, recommending binge-worthy content, or identifying power users.\n",
        "#\n",
        " 2.  **`flag_age_extreme`**:\n",
        "     *   **Why keep:** This flag primarily serves as a data quality indicator. Extreme ages (e.g., <10 or >100) are often data entry errors or represent a very small, potentially unrepresentative, segment of the user base. While it might be useful to flag these for data cleaning or to understand data quality issues, it's less likely to be a direct predictor of user behavior unless there's a specific hypothesis that these extreme (and likely erroneous) ages correlate with certain behaviors.\n",
        "     *   **Use case:** Primarily for data cleaning and outlier detection in user demographics. Could be used as a feature if the hypothesis is that users who enter nonsensical ages have different engagement patterns (e.g., bot accounts, or users intentionally trying to bypass age restrictions).\n",
        "#\n",
        " 3.  **`flag_duration_anomaly`**:\n",
        "     *   **Why keep:** This flag identifies movies with unusually short (less than 15 minutes) or long (more than 480 minutes) durations.\n",
        "         *   **Short durations:** Might indicate trailers, short films, or incorrectly recorded data.\n",
        "         *   **Long durations:** Might indicate documentaries, special features, or very long films.\n",
        "     *   **Use case:** Useful for understanding content characteristics. For recommendation systems, knowing if a movie is a short film versus a feature-length epic can be crucial for user satisfaction. It can also help identify content that might require different user engagement strategies or marketing approaches.\n",
        "#\n",
        " **Recommendation:**\n",
        "#\n",
        " *   **`flag_binge`** is likely the most valuable feature for predicting user behavior and engagement, as it directly captures a significant interaction pattern.\n",
        " *   **`flag_duration_anomaly`** is also valuable, especially for content-based recommendations or understanding content types.\n",
        " *   **`flag_age_extreme`** is more of a data quality flag. It might be kept if there's a strong hypothesis that these data errors correlate with specific user behaviors, but it's often better addressed through data cleaning first.\n",
        "#\n",
        " For a general-purpose model predicting user engagement or churn, `flag_binge` would be a strong candidate to include as a feature."
      ],
      "metadata": {
        "id": "Hiwyo7AtVeFN"
      },
      "id": "Hiwyo7AtVeFN"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qJJ72oFkEdp"
      },
      "source": [
        "## 6) Save & submit — What & Why\n",
        "Reproducibility: save artifacts and document decisions so others can rerun and audit."
      ],
      "id": "8qJJ72oFkEdp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBLmSK_QkEdp"
      },
      "source": [
        "### Build Prompt\n",
        "Generate a checklist (Markdown) students can paste at the end:\n",
        "- Save this notebook to the team Drive.\n",
        "- Export a `.sql` file with your DQ queries and save to repo.\n",
        "- Push notebook + SQL to the **team GitHub** with a descriptive commit.\n",
        "- Add a README with your `PROJECT_ID`, `REGION`, bucket, dataset, and today’s row counts.\n"
      ],
      "id": "SBLmSK_QkEdp"
    },
    {
      "cell_type": "code",
      "source": [
        "# Export all Data Quality SQL queries to a .sql file for reproducibility.\n",
        "import os\n",
        "\n",
        "project_id = os.environ['GOOGLE_CLOUD_PROJECT']\n",
        "\n",
        "sql_content = f\"\"\"\n",
        "-- Data Quality (DQ) Queries for Netflix Dataset\n",
        "-- Project ID: {project_id}\n",
        "\n",
        "-- 5.1 Missingness (users) - Query 1\n",
        "-- Calculate total rows and percentage of missing values for country, subscription_plan, and age.\n",
        "SELECT\n",
        "    COUNT(*) AS total_rows,\n",
        "    SUM(CASE WHEN country IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_missing_country,\n",
        "    SUM(CASE WHEN subscription_plan IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_missing_subscription_plan,\n",
        "    SUM(CASE WHEN age IS NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS percent_missing_age\n",
        "FROM\n",
        "    `{project_id}.netflix.users`;\n",
        "\n",
        "-- 5.1 Missingness (users) - Verification Query (Missingness percentages rounded)\n",
        "WITH base AS (\n",
        "  SELECT COUNT(*) n,\n",
        "         COUNTIF(country IS NULL) miss_country,\n",
        "         COUNTIF(subscription_plan IS NULL) miss_plan,\n",
        "         COUNTIF(age IS NULL) miss_age\n",
        "  FROM `{project_id}.netflix.users`\n",
        ")\n",
        "SELECT\n",
        "       ROUND(100*miss_country/n,2) AS pct_missing_country,\n",
        "       ROUND(100*miss_plan/n,2)   AS pct_missing_subscription_plan,\n",
        "       ROUND(100*miss_age/n,2)    AS pct_missing_age\n",
        "FROM base;\n",
        "\n",
        "-- 5.2 Duplicates (watch_history) - Query 1 (Report duplicate groups)\n",
        "SELECT user_id, movie_id, watch_date, device_type, COUNT(*) AS dup_count\n",
        "FROM `{project_id}.netflix.watch_history`\n",
        "GROUP BY user_id, movie_id, watch_date, device_type\n",
        "HAVING dup_count > 1\n",
        "ORDER BY dup_count DESC\n",
        "LIMIT 20;\n",
        "\n",
        "-- 5.2 Duplicates (watch_history) - Query 2 (Create deduplicated table)\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_dedup` AS\n",
        "SELECT * EXCEPT(rk) FROM (\n",
        "  SELECT h.*,\n",
        "         ROW_NUMBER() OVER (\n",
        "           PARTITION BY user_id, movie_id, watch_date, device_type\n",
        "           ORDER BY progress_percentage DESC, watch_duration_minutes DESC\n",
        "         ) AS rk\n",
        "  FROM `{project_id}.netflix.watch_history` h\n",
        ")\n",
        "WHERE rk = 1;\n",
        "\n",
        "-- 5.2 Duplicates (watch_history) - Verification Query (Before/after count)\n",
        "SELECT\n",
        "  (SELECT COUNT(*) FROM `{project_id}.netflix.watch_history`) AS raw_count,\n",
        "  (SELECT COUNT(*) FROM `{project_id}.netflix.watch_history_dedup`) AS dedup_count;\n",
        "\n",
        "-- 5.3 Outliers (minutes_watched) - Query 1 (Compute IQR bounds and % outliers)\n",
        "WITH\n",
        "  quantiles AS (\n",
        "    SELECT\n",
        "      APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(1)] AS q1,\n",
        "      APPROX_QUANTILES(watch_duration_minutes, 4)[OFFSET(3)] AS q3\n",
        "    FROM\n",
        "      `{project_id}.netflix.watch_history_dedup`\n",
        "  ),\n",
        "  bounds AS (\n",
        "    SELECT\n",
        "      q1,\n",
        "      q3,\n",
        "      q1 - 1.5 * (q3 - q1) AS lower_bound,\n",
        "      q3 + 1.5 * (q3 - q1) AS upper_bound\n",
        "    FROM\n",
        "      quantiles\n",
        "  )\n",
        "SELECT\n",
        "  COUNT(*) AS total_rows,\n",
        "  SUM(CASE WHEN watch_duration_minutes < b.lower_bound OR watch_duration_minutes > b.upper_bound THEN 1 ELSE 0 END) AS outlier_count,\n",
        "  SUM(CASE WHEN watch_duration_minutes < b.lower_bound OR watch_duration_minutes > b.upper_bound THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS outlier_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_dedup`, bounds b;\n",
        "\n",
        "-- 5.3 Outliers (minutes_watched) - Query 2 (Create robust table with capped values)\n",
        "CREATE OR REPLACE TABLE `{project_id}.netflix.watch_history_robust` AS\n",
        "SELECT\n",
        "  *, -- Select all original columns\n",
        "  -- Cap watch_duration_minutes at P01 and P99 values (hardcoded from previous execution for example)\n",
        "  CASE\n",
        "    WHEN watch_duration_minutes < 4.4 THEN 4.4 -- Replace with actual P01_val\n",
        "    WHEN watch_duration_minutes > 366.0 THEN 366.0 -- Replace with actual P99_val\n",
        "    ELSE watch_duration_minutes\n",
        "  END AS watch_duration_minutes_capped\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_dedup`;\n",
        "\n",
        "-- 5.3 Outliers (minutes_watched) - Verification Query (Min/Median/Max before vs after capping)\n",
        "SELECT\n",
        "  'Before Capping' AS stage,\n",
        "  MIN(watch_duration_minutes) AS min_minutes,\n",
        "  APPROX_QUANTILES(watch_duration_minutes, 2)[OFFSET(1)] AS median_minutes,\n",
        "  MAX(watch_duration_minutes) AS max_minutes\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_dedup`\n",
        "\n",
        "UNION ALL\n",
        "\n",
        "SELECT\n",
        "  'After Capping' AS stage,\n",
        "  MIN(watch_duration_minutes_capped) AS min_minutes,\n",
        "  APPROX_QUANTILES(watch_duration_minutes_capped, 2)[OFFSET(1)] AS median_minutes,\n",
        "  MAX(watch_duration_minutes_capped) AS max_minutes\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_robust`;\n",
        "\n",
        "-- 5.4 Business anomaly flags - Query 1 (Summarize flag_binge)\n",
        "SELECT\n",
        "  COUNT(*) AS total_sessions,\n",
        "  SUM(CASE WHEN watch_duration_minutes_capped > (8 * 60) THEN 1 ELSE 0 END) AS binge_sessions_count,\n",
        "  SUM(CASE WHEN watch_duration_minutes_capped > (8 * 60) THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS binge_sessions_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.watch_history_robust`;\n",
        "\n",
        "-- 5.4 Business anomaly flags - Query 2 (Summarize flag_age_extreme)\n",
        "SELECT\n",
        "  COUNT(*) AS total_users,\n",
        "  SUM(CASE WHEN age < 10 OR age > 100 THEN 1 ELSE 0 END) AS extreme_age_count,\n",
        "  SUM(CASE WHEN age < 10 OR age > 100 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS extreme_age_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.users`\n",
        "WHERE age IS NOT NULL;\n",
        "\n",
        "-- 5.4 Business anomaly flags - Query 3 (Summarize flag_duration_anomaly)\n",
        "SELECT\n",
        "  COUNT(*) AS total_movies,\n",
        "  SUM(CASE WHEN duration_minutes < 15 OR duration_minutes > 480 THEN 1 ELSE 0 END) AS duration_anomaly_count,\n",
        "  SUM(CASE WHEN duration_minutes < 15 OR duration_minutes > 480 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS duration_anomaly_percentage\n",
        "FROM\n",
        "  `{project_id}.netflix.movies`\n",
        "WHERE duration_minutes IS NOT NULL;\n",
        "\n",
        "-- 5.4 Business anomaly flags - Verification Query (Compact summary)\n",
        "WITH\n",
        "  binge_flags AS (\n",
        "    SELECT\n",
        "      'flag_binge' AS flag_name,\n",
        "      SUM(CASE WHEN watch_duration_minutes_capped > (8 * 60) THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS pct_of_rows\n",
        "    FROM\n",
        "      `{project_id}.netflix.watch_history_robust`\n",
        "  ),\n",
        "  age_flags AS (\n",
        "    SELECT\n",
        "      'flag_age_extreme' AS flag_name,\n",
        "      SUM(CASE WHEN age < 10 OR age > 100 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS pct_of_rows\n",
        "    FROM\n",
        "      `{project_id}.netflix.users`\n",
        "    WHERE age IS NOT NULL\n",
        "  ),\n",
        "  duration_flags AS (\n",
        "    SELECT\n",
        "      'flag_duration_anomaly' AS flag_name,\n",
        "      SUM(CASE WHEN duration_minutes < 15 OR duration_minutes > 480 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS pct_of_rows\n",
        "    FROM\n",
        "      `{project_id}.netflix.movies`\n",
        "    WHERE duration_minutes IS NOT NULL\n",
        "  )\n",
        "SELECT * FROM binge_flags\n",
        "UNION ALL\n",
        "SELECT * FROM age_flags\n",
        "UNION ALL\n",
        "SELECT * FROM duration_flags;\n",
        "\"\"\"\n",
        "\n",
        "# Write the SQL content to a file\n",
        "with open('dq_queries.sql', 'w') as f:\n",
        "    f.write(sql_content)\n",
        "\n",
        "print(\"All DQ queries have been exported to dq_queries.sql\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVlfTOyyWg9G",
        "outputId": "5abc7809-eb49-4384-f0c9-fa641df0a6d5"
      },
      "id": "KVlfTOyyWg9G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All DQ queries have been exported to dq_queries.sql\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3ILBkZTkEdp"
      },
      "source": [
        "## Grading rubric (quick)\n",
        "- Profiling completeness (30)  \n",
        "- Cleaning policy correctness & reproducibility (40)  \n",
        "- Reflection/insight (20)  \n",
        "- Hygiene (naming, verification, idempotence) (10)\n"
      ],
      "id": "G3ILBkZTkEdp"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
